{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp pcitpy\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we review the requirements for and toolbox functions supporting the configuration of parameters for curve-fitting with P-CIT before execution of the core `importance_sampler` procedure. Most language-agnostic guidance is recreated directly from the [P-CIT Toolbox Manual](https://github.com/PrincetonUniversity/p-cit-toolbox) provided by Annamalai Natarajan, Samuel Gershman, Luis Piloto, Greg Detre, and Kenneth Norman with Princeton University. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of the data matrix that holds the information required for the analysis is $T \\times 6$.\n",
    "$T$ here corresponds to the total number of entries (over all subjects, all items, all repetitions) that\n",
    "you would like analyzed. All entries in the data matrix will need to be numeric (exception-the\n",
    "predictor variable values can be set to NaN; see discussion of baseline items below). The 6 columns\n",
    "correspond to the _subject id_, _sample number_, _category_, _predictor variable_, _dependent variable_ and _net effect cluster_ respectively. Before we explain each of these columns in detail, let us look at an example data matrix. Table 1 in the P-CIT Toolbox Manual shows data entries from a variant of the think/no-think dataset. The entries in the table correspond to no-think and baseline items from two categories, face and scene. The predictor variable here is the difference between relevant-category and irrelevant-category classifier readouts. The voxels that were fed into the classifier were selected using a bilateral two-region (fusiform gyrus and parahippocampal gyrus) mask. In this variant of the dataset the\n",
    "classifier, was trained on four categories (face, scene, car, shoe) and tested on two categories (face,\n",
    "scene) only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Column 1** -_subject id_: Within a data matrix, each subject should be assigned a unique subject ID. From the example data matrix you can see that subject IDs are the same for all entries within a subject.\n",
    "\n",
    "- **Column 2** -_trial number_: The trial number must be unique within each subject but can be repeated across subjects. In the example data matrix, the trial numbers go from 1 through 106 for subject 1 before being repeated for subject 2.\n",
    "\n",
    "- **Column 3** -_category_: Category numbers can be used to represent different conditions within the experiment. The curve-fitting procedure can fit a curve to just one category if need be. If this is irrelevant to your dataset, just set it to -1. From the example data matrix the entries are divided up into to either category 1 (face trials) or category 2 (scene trials).\n",
    "\n",
    "- **Column 4** -_predictor variable_: This column should be populated with predictor variable data from your experiments (e.g., classifier outputs, reaction times - whatever information it is that you are using to predict the dependent variable). In the example data matrix, we are using the difference between relevant-category and irrelevant-category classifier readouts as our predictor variable. Also, note some entries (trial numbers 105, 106) in the example data matrix are NaNs. These entries correspond to the baseline items, where no predictor variable information is available (see the \"Computing importance weights for individual curves\" section of the main paper, and the \"Anchoring the vertical position of the curve using baseline items\" section of the supplementary materials). When computing the net effects these baseline items, by default, have their net effects set to zero.\n",
    "\n",
    "- **Column 5** -_dependent variable_: You can populate this column with dependent variable data from your experiments. In the example data matrix, the dependent variable is Bernoulli distributed (i.e., 0 or 1). The toolbox is equipped to handle normally distributed continuous dependent variables as well; refer to Section 6.3 of the Manual. \n",
    "\n",
    "- **Column 6** -_net effect cluster_ : This tells the toolbox which trials to group together when computing net effects. In the think/no-think experiment, all 12 repetitions of a given no-think item are grouped within the same net effect cluster. Note that, for all trials within a given net effect cluster, the predictor variable values can be different across these trials, but they all must share the same dependent variable value (e.g., in the think/no-think experiment, each repetition of a given no-think item was associated with a different level of classifier evidence, but all of these repetitions were associated with same dependent variable value - the item was either remembered correctly or incorrectly on the final test). Each net effect cluster should have its own unique identifier value (e.g., in the think/no-think dataset, all of the repetitions of the first no-think item from the first participant were assigned a net effect cluster value of 1; all of the repetitions of the second no-think item from the first participant were assigned a net effect cluster value of 2; and so on). Note also that the trials belonging to a given net effect cluster do not need to be contiguous to one another in the matrix (see Table 1 in the P-CIT Toolbox Manual).\n",
    "\n",
    "Table 1 in the P-CIT Toolbox Manual depicts a variant of the think/no-think analysis in which 8 no-think items were repeated 12 times and 10 baseline items were assigned one row each; this brings the total number of rows per subject to 106 (8 x 12 + 10 x 1). In the think/no-think paper, authors analyzed data from a total of 26 subjects, hence the total number of rows in the data matrix for this variant of the think/no-think analysis is 2756. They discuss other variants of the think/no-think analysis in Section 4.12 of the Manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "from pcitpy.pcitpy import importance_sampler\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "\n",
    "\n",
    "def run_importance_sampler(analysis_settings=None, run_sampler=True):\n",
    "    \"\"\"Sets up the data matrix (number of samples x 6 columns) and the \n",
    "    `analysis_settings` dictionary with algorithm parameters before starting \n",
    "    the importance sampler.\n",
    "    \n",
    "    This is the driver routine that you will use to load your data matrix and \n",
    "    also set parameters for the curve-fitting procedure. Running the function \n",
    "    will initialize the parameters and initiate the core `importance_sampler` \n",
    "    function of the toolbox.\n",
    "\n",
    "    **Arguments**:  \n",
    "    - analysis_settings: optional parameter configuration as dictionary. \n",
    "        Default parameters will be used otherwise.\n",
    "    - run_sampler: if True, runs importance_sampler using specified settings. \n",
    "        Otherwise returns loaded data matrix and analysis_settings dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    if analysis_settings is None:\n",
    "        \n",
    "        # Populating the analysis_settings struct with algorithm settings\n",
    "        analysis_settings = {}\n",
    "        analysis_settings['analysis_id'] = 'my_analysis_id'  # analysis_id: specifies the target directory\n",
    "        analysis_settings['em_iterations'] = 20  # Number of expectation maximization iterations\n",
    "        analysis_settings['particles'] = 100000  # Number of particles to be used in the importance sampling algorithm\n",
    "        analysis_settings['curve_type'] = 'horz_indpnt'  # Name of family of curves to be used. Refer to family_of_curves\n",
    "\n",
    "        # Name of the distribution (and the default canonical link function which maps the predictor variable to the DV)\n",
    "        analysis_settings['distribution'] = 'bernoulli'\n",
    "        analysis_settings['dist_specific_params'] = {}  # For normal distribution the additional parameter is sigma\n",
    "        analysis_settings['dist_specific_params']['sigma'] = 1\n",
    "\n",
    "        analysis_settings['beta_0'] = 0  # Initializing beta_0 for linear predictor\n",
    "        analysis_settings['beta_1'] = 1  # Initializing beta_1 for linear predictor\n",
    "        analysis_settings['tau'] = 0.05  # Specifies the radius to sample curves in the curve space\n",
    "\n",
    "        # Specifies if the analyses will need to run on a specific category. Vector length should be greater than 0.\n",
    "        # For instance [2] will cause the analyses to be run only on the second category\n",
    "        # [] will run the analyses on all categories\n",
    "        analysis_settings['category'] = []\n",
    "\n",
    "        # specifies how many std dev away from group mean will the predictor variable outliers need to be dropped\n",
    "        analysis_settings['drop_outliers'] = 3\n",
    "\n",
    "        # if TRUE, the independent variables will be z-scored within each subject\n",
    "        analysis_settings['zscore_within_subjects'] = False\n",
    "\n",
    "        # Registering which column in the data matrix is carrying which piece of information\n",
    "        analysis_settings['data_matrix_columns'] = {}\n",
    "        analysis_settings['data_matrix_columns']['subject_id'] = 0\n",
    "        analysis_settings['data_matrix_columns']['trials'] = 1\n",
    "        analysis_settings['data_matrix_columns']['category'] = 2\n",
    "        analysis_settings['data_matrix_columns']['predictor_var'] = 3\n",
    "        analysis_settings['data_matrix_columns']['dependent_var'] = 4\n",
    "        analysis_settings['data_matrix_columns']['net_effect_clusters'] = 5\n",
    "\n",
    "        analysis_settings['resolution'] = 4  # Denotes the resolution in which the data will be processed\n",
    "\n",
    "        # Denotes the number of chunks you plan to partition the trials x particles matrix.\n",
    "        # An example chunk size will be 2 for a 3000 x 50,000 matrix\n",
    "        analysis_settings['particle_chunks'] = 2\n",
    "        analysis_settings['bootstrap'] = False  # indicates that this run is a bootstrap run\n",
    "        analysis_settings['bootstrap_run'] = -1  # if non-negative, specify bootstrap sample number unique for each sample\n",
    "\n",
    "        analysis_settings['scramble'] = False  # indicates whether this run is a scramble run\n",
    "        analysis_settings['scramble_run'] = -1  # if non-negative, specify bootstrap sample number unique for each sample\n",
    "        analysis_settings['scramble_style'] = -1  # choosing the appropriate scramble option from three options below\n",
    "        if analysis_settings['scramble_style'] > 0:\n",
    "            if analysis_settings['scramble_style'] == 1:\n",
    "                analysis_settings['scramble_style'] = 'within_subjects_within_categories'\n",
    "            elif analysis_settings['scramble_style'] == 2:\n",
    "                analysis_settings['scramble_style'] = 'within_subjects_across_categories'\n",
    "            elif analysis_settings['scramble_style'] == 3:\n",
    "                analysis_settings['scramble_style'] = 'across_subjects_across_categories'\n",
    "            else:\n",
    "                raise ValueError('Invalid scramble style given!')\n",
    "\n",
    "    # %%%%%%%%%%%%%%%%%%%%\n",
    "    # Reading in the data\n",
    "    # %%%%%%%%%%%%%%%%%%%%\n",
    "    # The lines below load the simulated data into the raw_data matrix.\n",
    "    # Replace these lines of the code with code to load your actual data\n",
    "\n",
    "    results_dir = os.path.join(os.getcwd(), 'results')\n",
    "    data_path = os.path.join(results_dir, analysis_settings['analysis_id'],\n",
    "                             analysis_settings['analysis_id'] + '.mat')\n",
    "    data = loadmat(data_path)['data']\n",
    "    raw_data = simulated_data['raw_data']\n",
    "    if run_sampler:\n",
    "        importance_sampler(raw_data, analysis_settings)\n",
    "    else:\n",
    "        return data, analysis_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(run_importance_sampler, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its original implementation in MATLAB, as opposed to accepting any \n",
    "input parameter configuration, parameters were specified within \n",
    "`run_importance_sampler` by altering variable assignments in its code. For \n",
    "a similar workflow, you could configure your own copy of the function \n",
    "(click \"source\" above to locate the current implementation) to specify \n",
    "parameters and run the function. Alternatively, you can provide an \n",
    "`analysis_settings` dictionary object as an argument to \n",
    "`run_importance_sampler`, specifying parameters as a set of key-value \n",
    "pairs.\n",
    "\n",
    "Refer to Table 2 in the P-CIT Toolbox Manual for parameters, description, \n",
    "example usage and default settings. All these parameters are initialized \n",
    "in `run_importance_sampler`. The default settings correspond to the \n",
    "settings that were used to do curve-fitting on scene, no-think trials in \n",
    "the Detre et al. paper. Settings related to bootstrap and scramble \n",
    "analyses are explained in more detail in Sections 4.8 and 4.9 \n",
    "respectively of the P-CIT Toolbox Manual."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
