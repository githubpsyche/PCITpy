{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run_importance_sampler\n",
    "sets up the data matrix (number of samples x 6 columns) and the 'analysis_settings' struct with algorithm parameters\n",
    "\n",
    "**USAGE**:\n",
    "[fillhandle,msg]=jbfill(xpoints,upper,lower,color,edge,add,transparency)\n",
    "\n",
    "**INPUTS**:\n",
    "- None\n",
    "\n",
    "**OUTPUTS**:\n",
    "- None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_importance_sampler():\n",
    "    analysis_settings = {} # Creating a dictionary\n",
    "    analysis_settings['analysis_id'] = 'my_analysis_id' # analysis_id: specifies the target directory\n",
    "    analysis_settings['em_iterations'] = 20 # Number of expectation maximization iterations\n",
    "    analysis_settings['particles'] = 100000 # Number of particles to be used in the importance sampling algorithm\n",
    "    analysis_settings['curve_type'] = 'horz_indpnt' # Name of the family of curves to be used. Refer to the family_of_curves.m file for more info\n",
    "    analysis_settings['distribution'] = 'bernoulli' # Name of the distribution (and the default canonical link function which maps the predictor variable to the dependent variable)\n",
    "    analysis_settings['dist_specific_params'] = {} # For normal distribution the additional parameter is sigma. We pass in sigma here.\n",
    "    analysis_settings['dist_specific_params']['sigma'] = 1\n",
    "    \n",
    "    analysis_settings['beta_0'] = 0 # Initializing beta_0 for linear predictor\n",
    "    analysis_settings['beta_1'] = 1 # Initializing beta_1 for linear predictor\n",
    "    analysis_settings['tau'] = 0.05 # Specifies the radius to sample curves in the curve space\n",
    "    analysis_settings['category'] = None # Specifies if the analyses will need to run on a specific category. Vector length Should be greater than 0. For instance [2] will cause the analyses to be run only on the second category; [] will run the analyses on all categories\n",
    "\n",
    "    analysis_settings['drop_outliers'] = 3 # specifies how many std dev away from group mean will the predictor variable outliers need to be dropped\n",
    "    analysis_settings['zscore_within_subjects'] = False # if TRUE, the independednt variables will be zscored within each suibject\n",
    "\n",
    "    # Registering which column in the data matrix is carrying which piece of information\n",
    "    analysis_settings['data_matrix_columns'] ={}\n",
    "    analysis_settings['data_matrix_columns']['subject_id'] = 1\n",
    "    analysis_settings['data_matrix_columns']['trials'] = 2\n",
    "    analysis_settings['data_matrix_columns']['category'] = 3\n",
    "    analysis_settings['data_matrix_columns']['predictor_var'] = 4\n",
    "    analysis_settings['data_matrix_columns']['dependent_var'] = 5\n",
    "    analysis_settings['data_matrix_columns']['net_effect_clusters'] = 6\n",
    "    \n",
    "    analysis_settings['resolution'] = 4 # Denotes the resolution in which the data will be processed\n",
    "    analysis_settings['particle_chunks'] = 2 # Denotes the number of chunks you plan to partition the trials x particles matrix. An example chunk size will be 2 for a 3000 x 50,000 matrix\n",
    "\n",
    "    analysis_settings['bootstrap'] = False # indicates that this run is a bootstrap run\n",
    "    analysis_settings['bootstrap_run'] = -1 # will need to specify a bootstrap sample number. This will need to be unique for each sample\n",
    "\n",
    "    analysis_settings['scramble'] = False # indicates that this run is a scramble run\n",
    "    analysis_settings['scramble_run'] = -1 # will need to specify a scramble sample number. This will need to be unique for each sample\n",
    "    analysis_settings['scramble_style'] = -1 # choosing the appropriate scramble option from three options below\n",
    "    if analysis_settings['scramble_style'] > 0:\n",
    "        if analysis_settings['scramble_style'] == 1:\n",
    "            analysis_settings['scramble_style'] = 'within_subjects_within_categories'\n",
    "        elif analysis_settings['scramble_style'] == 2:\n",
    "            analysis_settings['scramble_style'] = 'within_subjects_across_categories'\n",
    "        elif analysis_settings['scramble_style'] == 3:\n",
    "            analysis_settings['scramble_style'] = 'across_subjects_across_categories'\n",
    "        else:\n",
    "            raise ValueError('Invalid scramble style given!')\n",
    "\n",
    "    #%%%%%%%%%%%%%%%%%%%%\n",
    "    # Reading in the data\n",
    "    #%%%%%%%%%%%%%%%%%%%%\n",
    "    # The three lines below load the simulated data into the raw_data matrix. Replace these two lines of the code with code to load your actual data\n",
    "    \n",
    "    from scipy import io\n",
    "    results_dir = os.path.join(os.getcwd(), 'results')\n",
    "    fileName = os.path.join(results_dir, analysis_settings['analysis_id'], \n",
    "                analysis_settings['analysis_id'] + '_simulated_data' + '.mat')\n",
    "    simulated_data = scipy.io.loadmat(fileName,squeeze_me=True, struct_as_record=False)\n",
    "    # assuming loaded data has raw_data variable name\n",
    "    raw_data = simulated_data['raw_data']\n",
    "    importance_sampler(raw_data, analysis_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
