{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# family_of_distributions\n",
    "For each of the family of distributions this script performs specific computations like number of pdf/pmf, etc\n",
    "\n",
    "**USAGE**:\n",
    "[OUTPUT] = FAMILY_OF_DISTRIBUTIONS(DISTRIBUTION_NAME, GET_INFO, VARARGIN)\n",
    "\n",
    "**INPUTS**:\n",
    "- distribution_name: distribution name, string, e.g. 'bernoulli', 'normal'\n",
    "- get_info: Cues for specific information / computation, string, e.g. 'get_nParams'\n",
    "- varargin: Is either empty or has arguments depending on the computation\n",
    "\n",
    "**OUTPUTS**:\n",
    "- output: Holds the output of all computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def family_of_distributions(distribution_name, get_info, varargin):\n",
    "    if distribution_name is 'bernoulli':\n",
    "        return bernoulli_distribution(get_info, varargin)\n",
    "    elif distribution_name is 'normal':\n",
    "        return normal_distribution(get_info, varargin)\n",
    "    else:\n",
    "        raise ValueError('Invalid distribution!')\n",
    "\n",
    "def bernoulli_distribution(get_info, input_params):\n",
    "    if get_info is 'compute_densities': # --> (1), Compute the log densities. NOTE: We compute the log(probability function)\n",
    "        if len(input_params) <= 1:\n",
    "            raise ValueError('Missing input parameters!')\n",
    "        z = input_params[0]\n",
    "        y = input_params[1]\n",
    "        \n",
    "        # Compute fz = 1 / (1 + exp(-z) - Logistic function\n",
    "        fz = np.divide(1, (1 + np.exp(-z)));\n",
    "        fz = max(fz, np.finfo(float).eps);\n",
    "        fz = min(fz, 1-np.finfo(float).eps);\n",
    "        \n",
    "        # Compute bern_log_pmf = p ^ k + (1 - p) ^ (1 - k). http://en.wikipedia.org/wiki/Bernoulli_distribution\n",
    "        # Here p = fz and k = y. Taking the log results in y x log(fz) + (1 - y) x log(1 - fz). This is written below in bsxfun syntax\n",
    "        return np.sum(np.multiply(log(fz), y) + np.multiply(log(1-fz), np.subtract(1, y)))\n",
    "\n",
    "    elif get_info is 'fminunc_both_betas':\n",
    "        if len(input_params) <= 1:\n",
    "            raise ValueError('Missing input parameters!')\n",
    "        return lambda betas: fminunc_bernoulli_both(betas, input_params[0], input_params[1], input_params[2])\n",
    "    else:\n",
    "        raise ValueError('Invalid operation!')\n",
    "        \n",
    "def fminunc_bernoulli_both(betas, w, net_effects, dependent_var):\n",
    "\n",
    "    # [F, G] = FMINUNC_BERNOULLI_BOTH(BETAS)\n",
    "    # \n",
    "    # Purpose\n",
    "    # \n",
    "    # To optimize logistic regression betas using cost function F\n",
    "    #  \n",
    "    # Input\n",
    "    #\n",
    "    # --betas: The current betas that were used to compute likelihoods\n",
    "    # --w: Weight vector that holds the normalized weights for P particles\n",
    "    # --net_effects: Predictor variable Matrix (number of trials x particles)\n",
    "    # --dependent_var: Dependent variable Matrix (number of trials x 1)\n",
    "    # \n",
    "    # Output\n",
    "    #\n",
    "    # --f: Scalar, Objective function\n",
    "    # --g: Vector of length 2 i.e. gradients with respect to beta_0 and beta_1\n",
    "    #\n",
    "    beta_0 = betas[0]\n",
    "    beta_1 = betas[1]\n",
    "    \n",
    "    z = np.multiply(beta_1, net_effects + beta_0)\n",
    "    fz = np.divide(1, 1+np.exp(-z))\n",
    "    if np.any(np.isinf(fz)):\n",
    "        raise ValueError('Inf in fz matrix!')\n",
    "    fz = max(fz, np.finfo(float).eps);\n",
    "    fz = min(fz, 1-np.finfo(float).eps);\n",
    "    \n",
    "    # Cost function\n",
    "    # We will need to maximize the betas but fminunc minimizes hence a -ve.\n",
    "    # Here we compute the log pmf over all trials and then component multiply by the weights and then sum them up over all particles\n",
    "    f = -np.sum(np.multiply(w, np.sum(np.multiply(np.log(fz),dependent_var) + np.multiply(np.log(1-fz), np.subtract(1, dependent_var)))))\n",
    "    \n",
    "    # Here we take the partial derivative of log pmf over beta_0 and beta_1 respectively, component multiply by the weights and sum them up over all paricles\n",
    "    g = []\n",
    "    g.append(-np.sum(np.multiply(w, np.sum(np.subtract(dependent_var, np.divide(np.exp(z), 1-np.exp(z)))))))\n",
    "    g.append(-np.sum(np.multiply(w, np.sum(np.multiply(net_effects, dependent_var) - np.divide(np.multiply(net_effects, np.exp(z)), 1+np.exp(z))))))\n",
    "    if np.any(np.isinf(g)):\n",
    "        raise ValueError('Inf in partial derivative!')\n",
    "    if np.any(np.isnan(g)):\n",
    "        raise ValueError('NaN in partial derivative!')\n",
    "    \n",
    "    return f,g\n",
    "\n",
    "def normal_distribution(get_info, input_params):\n",
    "    if get_info is 'compute_densities':\n",
    "        if len(input_params) <= 2:\n",
    "            raise ValueError('Missing input parameters!')\n",
    "        \n",
    "        mu = input_params[0]\n",
    "        y = input_params[1]\n",
    "        dist_specific_params = input_params[2]\n",
    "        sigma = dist_specific_params['sigma']\n",
    "        \n",
    "        # Compute log_pdf http://en.wikipedia.org/wiki/Normal_distribution\n",
    "        return np.sum(np.subtract(np.multiply(np.divide(1, np.power(sigma, 2)), np.subtract(np.multiply(y, mu), np.add(np.multiply(.5,np.power(mu, 2)),np.multiply(.5,np.power(y, 2))))),\n",
    "                                  np.multiply(.5, np.log(np.multiply(2, np.multiply(np.pi, np.power(sigma, 2)))))))\n",
    "    elif get_info is 'fminunc_both_betas':\n",
    "        if len(input_params) <= 3:\n",
    "            raise ValueError('Missing input parameters!')\n",
    "        \n",
    "        return lambda betas: fminunc_normal_both(betas, input_params[0], input_params[1], input_params[2], input_params[3])\n",
    "    else:\n",
    "        raise ValueError('Invalid operation!')\n",
    "        \n",
    "def fminunc_normal_both(betas, w, net_effects, dependent_var, dist_specific_params):\n",
    "    \n",
    "    # [F, G] = FMINUNC_NORMAL_BOTH(BETAS)\n",
    "    # \n",
    "    # Purpose\n",
    "    # \n",
    "    # To optimize logistic regression betas using cost function F\n",
    "    #  \n",
    "    # Input\n",
    "    #\n",
    "    # --betas: The current betas that were used to compute likelihoods\n",
    "    # --w: Weight vector that holds the normalized weights for P particles\n",
    "    # --net_effects: Predictor variable Matrix (number of trials x particles)\n",
    "    # --dependent_var: Dependent variable Matrix (number of trials x 1)\n",
    "    # --sigma: Used to specify variance in the Normal distribution\n",
    "    # \n",
    "    # Output\n",
    "    #\n",
    "    # --f: Scalar, Objective function\n",
    "    # --g: Vector of length 2 i.e. gradients with respect to beta_0 and beta_1\n",
    "    \n",
    "    beta_0 = betas[0]\n",
    "    beta_1 = betas[1]\n",
    "    sigma = dist_specific_params['sigma']\n",
    "    \n",
    "    mu = np.multiply(beta_1, net_effects) + beta_0\n",
    "    \n",
    "    # Cost function\n",
    "    # We will need to maximize the betas but fminunc minimizes hence a -ve.\n",
    "    # Here we compute the log pdf over all trials and then component multiply by the weights and then sum them up over all particles\n",
    "    f = -np.sum(np.multiply(w, np.sum(np.subtract(np.multiply(np.divide(1,np.power(sigma, 2)), np.subtract(np.multiply(dependent_var, mu),np.add(np.multiply(.5,np.power(mu, 2)), np.multiply(.5,np.power(dependent_var,2))))),\n",
    "                                                  np.multiply(.5, np.log(np.multiply(2, np.multiply(np.pi, np.power(sigma, 2)))))))))\n",
    "    \n",
    "    # Here we take the partial derivative of log pdf over beta_0 and beta_1 respectively, component multiply by the weights and sum them up over all paricles\n",
    "    g = []\n",
    "    g.append(-np.sum(np.multiply(w, np.sum(np.multiply(np.divide(1, np.power(sigma,2)),np.subtract(dependent_var,np.add(beta_0, np.multiply(beta_1, net_effects))))))))\n",
    "    g.append(-np.sum(np.multiply(w, np.sum(np.multiply(np.divide(net_effects, np.power(sigma, 2)), np.subtract(dependent_var, np.add(beta_0, np.multiply(beta_1, net_effects))))))))\n",
    "    \n",
    "    if np.any(np.isinf(g)):\n",
    "        raise ValueError('Inf in partial derivative!')\n",
    "    if np.any(np.isnan(g)):\n",
    "        raise ValueError('NaN in partial derivative!')\n",
    "    \n",
    "    return f,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
