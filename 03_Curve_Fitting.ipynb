{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp pcitpy\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For description of the curve-fitting procedure, see the \"Estimating the plasticity curve\" section of\n",
    "main paper and the \"curve-fitting algorithm details\" section of the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "# helper functions from pcitpy\n",
    "import math\n",
    "from numba import vectorize, float64, int32, int64, njit\n",
    "from pcitpy.pcitpy import preprocessing_setup\n",
    "from pcitpy.family_of_curves import family_of_curves\n",
    "from pcitpy.common_to_all_curves import common_to_all_curves\n",
    "from pcitpy.family_of_distributions import family_of_distributions\n",
    "from pcitpy.helpers import likratiotest, truncated_normal\n",
    "\n",
    "# other dependencies\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "from scipy import optimize\n",
    "import scipy.io as sio\n",
    "from scipy import special\n",
    "\n",
    "\n",
    "def importance_sampler(raw_data, analysis_settings):\n",
    "    \"\"\"Recovers a curve that best explains the relationship between the predictor and dependent variables\n",
    "    \n",
    "    **Arguments**:\n",
    "    - raw_data: The data matrix (total number of trials x 6 columns). Refer to RUN_IMPORTANCE_SAMPLER()\n",
    "    - analysis_settings: A struct that holds algorithm relevant settings. Refer to RUN_IMPORTANCE_SAMPLER()\n",
    "\n",
    "    Saves a .mat file in `current_path/analysis_id/analysis_id_importance_sampler.mat`\n",
    "    \"\"\"\n",
    "    \n",
    "    time = datetime.datetime.now()\n",
    "    print('Start time {}/{} {}:{}'.format(time.month, time.day, time.hour, time.minute))\n",
    "\n",
    "    # Resetting the random number seed\n",
    "    random.seed()\n",
    "\n",
    "    # Preprocessing the data matrix and updating the analysis_settings struct with additional/missing information\n",
    "    preprocessed_data, ana_opt = preprocessing_setup(raw_data, analysis_settings)\n",
    "    del raw_data\n",
    "    del analysis_settings\n",
    "\n",
    "    # Housekeeping\n",
    "    importance_sampler = {}  # Creating the output struct\n",
    "    hold_betas_per_iter = np.full((ana_opt['em_iterations'] + 1, 2), np.nan)  # Matrix to hold betas over em iterations\n",
    "    exp_max_f_values = np.full((ana_opt['em_iterations'], 1), np.nan)  # Matrix to hold the f_values over em iterations\n",
    "    normalized_w = np.full((ana_opt['em_iterations'] + 1, ana_opt['particles']),\n",
    "                           np.nan)  # to hold the normalized weights\n",
    "    global tau\n",
    "    global bounds\n",
    "    global w\n",
    "    global net_effects\n",
    "    global dependent_var\n",
    "\n",
    "    # fetch parameters\n",
    "    tau = ana_opt['tau']  # Store the tau for convenience\n",
    "    bounds = family_of_curves(ana_opt['curve_type'], 'get_bounds')  # Get the curve parameter absolute bounds\n",
    "    nParam = family_of_curves(ana_opt['curve_type'], 'get_nParams')  # Get the number of curve parameters\n",
    "    hold_betas = [ana_opt['beta_0'], ana_opt['beta_1']]  # Store the betas into a vector\n",
    "\n",
    "    for em in range(ana_opt['em_iterations']):  # for every em iteration\n",
    "        hold_betas_per_iter[em, :] = hold_betas  # Store the logreg betas over em iterations\n",
    "        print('Betas: {}, {}'.format(hold_betas[0], hold_betas[1]))\n",
    "        print('EM Iteration: {}'.format(em))\n",
    "\n",
    "        # Initialize the previous iteration curve parameters, weight vector, net_effects and dependent_var matrices\n",
    "        # Matrix to hold the previous iteration curve parameters\n",
    "        prev_iter_curve_param = np.full((ana_opt['particles'], family_of_curves(ana_opt['curve_type'], 'get_nParams')),\n",
    "                                        np.nan)\n",
    "        w = np.full((ana_opt['particles']), np.nan)  # Vector to hold normalized weights\n",
    "\n",
    "        # Matrix to hold the predictor variables (taking net effects if relevant) over all particles\n",
    "        net_effects = np.full((len(ana_opt['net_effect_clusters']), ana_opt['particles']), np.nan)\n",
    "        dependent_var = np.array([])  # can't be initialized in advance as we don't know its length (dropping outliers)\n",
    "\n",
    "        # Sampling curve parameters\n",
    "        if em == 0:  # only for the first em iteration\n",
    "            param = common_to_all_curves(ana_opt['curve_type'], 'initial_sampling',\n",
    "                                         ana_opt['particles'], ana_opt['resolution'])  # Good old uniform sampling\n",
    "        else:  # for em iterations 2, 3, etc\n",
    "            # Sample curve parameters from previous iteration's curve parameters based on normalized weights\n",
    "            prev_iter_curve_param = param  # we need previous iteration's curve parameters to compute likelihood\n",
    "\n",
    "            # Here we sample curves (with repetitions) based on the weights\n",
    "            param = prev_iter_curve_param[random.choices(np.arange(ana_opt['particles']),\n",
    "                                                         k=ana_opt['particles'], weights=normalized_w[em - 1, :]), :]\n",
    "            # Add Gaussian noise since some curves are going to be identical due to the repetitions\n",
    "            # NOISE: Sample from truncated normal distribution using individual curve parameter bounds,\n",
    "            # mean = sampled curve parameters and sigma = tau\n",
    "            for npm in range(nParam):\n",
    "                param[:, npm] = truncated_normal(bounds[npm, 0], bounds[npm, 1],\n",
    "                                                 param[:, npm], tau, ana_opt['particles'])\n",
    "\n",
    "        # Check whether curve parameters lie within the upper and lower bounds\n",
    "        param = common_to_all_curves(ana_opt['curve_type'], 'check_if_exceed_bounds', param)\n",
    "        if ana_opt['curve_type'] == 'horz_indpnt':\n",
    "            # Check if the horizontal curve parameters are following the right trend i.e. x1 < x2\n",
    "            param = common_to_all_curves(ana_opt['curve_type'], 'sort_horizontal_params', param)\n",
    "\n",
    "            # Compute the likelihood over all subjects (i.e. log probability mass function if logistic regression)\n",
    "        #  This is where we use the chunking trick II\n",
    "        for ptl_idx in range(np.shape(ana_opt['ptl_chunk_idx'])[0]):\n",
    "            output_struct = family_of_curves(\n",
    "                ana_opt['curve_type'], 'compute_likelihood', ana_opt['net_effect_clusters'],\n",
    "                ana_opt['ptl_chunk_idx'][ptl_idx, 2],\n",
    "                param[int(ana_opt['ptl_chunk_idx'][ptl_idx, 0]):int(ana_opt['ptl_chunk_idx'][ptl_idx, 1]), :],\n",
    "                hold_betas, preprocessed_data,\n",
    "                ana_opt['distribution'], ana_opt['dist_specific_params'], ana_opt['data_matrix_columns'])\n",
    "\n",
    "            # Gather weights\n",
    "            w[int(ana_opt['ptl_chunk_idx'][ptl_idx, 0]):int(ana_opt['ptl_chunk_idx'][ptl_idx, 1])] = output_struct['w']\n",
    "\n",
    "            # Gather predictor variable\n",
    "            net_effects[:, int(ana_opt['ptl_chunk_idx'][ptl_idx, 0]):int(ana_opt['ptl_chunk_idx'][ptl_idx, 1])] = \\\n",
    "                output_struct['net_effects']\n",
    "            if ptl_idx == 0:\n",
    "                # Gather dependent variable only once, since it is the same across all ptl_idx\n",
    "                dependent_var = output_struct['dependent_var']\n",
    "\n",
    "        del output_struct\n",
    "        if np.any(np.isnan(w)):\n",
    "            raise ValueError('NaNs in normalized weight vector w!')\n",
    "\n",
    "        # Compute the p(theta) and q(theta) weights\n",
    "        if em > 0:\n",
    "            p_theta_minus_q_theta = compute_weights(\n",
    "                ana_opt['curve_type'], ana_opt['particles'], normalized_w[em - 1, :],\n",
    "                prev_iter_curve_param, param, ana_opt['wgt_chunks'], ana_opt['resolution'])\n",
    "            w += p_theta_minus_q_theta\n",
    "\n",
    "        w = np.exp(w - special.logsumexp(w))  # Normalize the weights using logsumexp to avoid numerical underflow\n",
    "        normalized_w[em, :] = w  # Store the normalized weights\n",
    "\n",
    "        # Optimize betas using fminunc\n",
    "        optimizing_function = family_of_distributions(ana_opt['distribution'], 'fminunc_both_betas', w, net_effects,\n",
    "                                                      dependent_var, ana_opt['dist_specific_params'])\n",
    "\n",
    "        result = optimize.minimize(optimizing_function, np.array(hold_betas), jac=True,\n",
    "                                   options={'disp': True, 'return_all': True})\n",
    "        hold_betas = result.x\n",
    "        f_value = result.fun\n",
    "\n",
    "        exp_max_f_values[em] = f_value  # gather the f_values over em iterations\n",
    "\n",
    "    hold_betas_per_iter[em + 1, :] = hold_betas  # Store away the last em iteration betas\n",
    "    print('>>>>>>>>> Final Betas: {}, {} <<<<<<<<<'.format(hold_betas[0], hold_betas[1]))\n",
    "\n",
    "    # Flipping the vertical curve parameters if beta_1 is negative\n",
    "    importance_sampler['flip'] = False\n",
    "    neg_beta_idx = hold_betas[1] < 0\n",
    "    if neg_beta_idx:\n",
    "        print('!!!!!!!!!!!!!!!!!!!! Beta 1 is flipped !!!!!!!!!!!!!!!!!!!!')\n",
    "        hold_betas[1] = hold_betas[1] * -1\n",
    "        param = common_to_all_curves(ana_opt['curve_type'], 'flip_vertical_params', param)\n",
    "        importance_sampler['flip'] = True\n",
    "\n",
    "    w = np.full((ana_opt['particles']), np.nan)  # Clearing the weight vector\n",
    "\n",
    "    # Used for a likelihoods ratio test to see if our beta1 value is degenerate\n",
    "    w_null_hypothesis = np.full((ana_opt['particles']), np.nan)\n",
    "\n",
    "    # The null hypothesis for the likelihoods ratio test states that our model y_hat = beta_0 + beta_1 * predictor\n",
    "    # variable is no different than the simpler model y_hat = beta_0 + beta_1 * predictor variable WHERE BETA_1 =\n",
    "    # ZERO i.e. our model is y_hat = beta_0\n",
    "    null_hypothesis_beta = [hold_betas[0], 0]\n",
    "\n",
    "    for ptl_idx in range(np.shape(ana_opt.ptl_chunk_idx)[0]):\n",
    "        output_struct = family_of_curves(\n",
    "            ana_opt['curve_type'], 'compute_likelihood', ana_opt['net_effect_clusters'],\n",
    "            ana_opt['ptl_chunk_idx'][ptl_idx, 3],\n",
    "            param[ana_opt['ptl_chunk_idx'][ptl_idx, 1]:ana_opt['ptl_chunk_idx'][ptl_idx, 2], :], hold_betas,\n",
    "            preprocessed_data,\n",
    "            ana_opt['distribution'], ana_opt['dist_specific_params'], ana_opt['data_matrix_columns'])\n",
    "        w[ana_opt['ptl_chunk_idx'][ptl_idx, 1]:ana_opt['ptl_chunk_idx'][ptl_idx, 2]] = output_struct['w']\n",
    "\n",
    "    # this code computes the log likelihood of the data under the null hypothesis i.e. using null_hypothesis_beta\n",
    "    # instead of hold_betas -- it's \"lazy\" because, unlike the alternative hypothesis, we don't have to compute the\n",
    "    # data likelihood for each particle because it's exactly the same for each particle (b/c compute_likelihood uses\n",
    "    # z = beta_1 * x + beta_0, but (recall that our particles control the value of x in this equation) beta_1 is zero\n",
    "    # for the null hypothesis) that's why we pass in the zero vector representing a single particle with irrelevant\n",
    "    # weights so we don't have to do it for each particle unnecessarily\n",
    "    output_struct_null_hypothesis_lazy = family_of_curves(\n",
    "        ana_opt['curve_type'], 'compute_likelihood', ana_opt['net_effect_clusters'], 1, [0, 0, 0, 0, 0, 0],\n",
    "        null_hypothesis_beta, preprocessed_data, ana_opt['distribution'], ana_opt['dist_specific_params'],\n",
    "        ana_opt['data_matrix_columns'])\n",
    "    data_likelihood_null_hypothesis = output_struct_null_hypothesis_lazy['w']\n",
    "    data_likelihood_alternative_hypothesis = w\n",
    "\n",
    "    w = w + p_theta_minus_q_theta\n",
    "    if np.any(np.isnan(w)):\n",
    "        raise ValueError('NaNs in normalized weight vector w!')\n",
    "\n",
    "    w = np.exp(w - special.logsumexp(w))  # Normalize the weights using logsumexp to avoid numerical underflow\n",
    "    normalized_w[em + 1, :] = w  # Store the normalized weights\n",
    "\n",
    "    # Added for debugging chi-sq, might remove eventually\n",
    "    importance_sampler['data_likelihood_alternative_hypothesis'] = data_likelihood_alternative_hypothesis\n",
    "    importance_sampler['data_likelihood_null_hypothesis'] = data_likelihood_null_hypothesis\n",
    "\n",
    "    # we calculate the data_likelihood over ALL particles by multiplying the data_likelihood for each particle by\n",
    "    # that particle's importance weight\n",
    "    dummy_var, importance_sampler['likratiotest'] = likratiotest(\n",
    "        w * np.transpose(data_likelihood_alternative_hypothesis), data_likelihood_null_hypothesis, 2, 1)\n",
    "\n",
    "    if np.any(np.isnan(normalized_w)):\n",
    "        raise ValueError('NaNs in normalized weights vector!')\n",
    "    if np.any(np.isnan(exp_max_f_values)):\n",
    "        raise ValueError('NaNs in Expectation maximilzation fval matrix!')\n",
    "    if np.any(np.isnan(hold_betas_per_iter)):\n",
    "        raise ValueError('NaNs in hold betas matrix!')\n",
    "\n",
    "    importance_sampler['normalized_weights'] = normalized_w\n",
    "    importance_sampler['exp_max_fval'] = exp_max_f_values\n",
    "    importance_sampler['hold_betas_per_iter'] = hold_betas_per_iter\n",
    "    importance_sampler['curve_params'] = param\n",
    "    importance_sampler['analysis_settings'] = ana_opt\n",
    "\n",
    "    if ana_opt['bootstrap']:\n",
    "        sio.savemat('{}/{}_b{}_importance_sampler.mat'.format(ana_opt['target_dir'], ana_opt['analysis_id'],\n",
    "                                                              ana_opt['bootstrap_run']),\n",
    "                    {'importance_sampler': importance_sampler})\n",
    "    elif ana_opt['scramble']:\n",
    "        sio.savemat('{}/{}_s{}_importance_sampler.mat'.format(ana_opt['target_dir'], ana_opt['analysis_id'],\n",
    "                                                              ana_opt['scramble_run']),\n",
    "                    {'importance_sampler': importance_sampler})\n",
    "    else:\n",
    "        sio.savemat('{}/{}_importance_sampler.mat'.format(ana_opt['target_dir'], ana_opt['analysis_id']),\n",
    "                    {'importance_sampler': importance_sampler})\n",
    "    print('Results are stored in be stored in {}'.format(ana_opt['target_dir']))\n",
    "\n",
    "    time = datetime.datetime.now()\n",
    "    print('Finish time {}/{} {}:{}'.format(time.month, time.day, time.hour, time.minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(importance_sampler, title_level=2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
