{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importance_sampler\n",
    "Recovers a curve that best explains the relationship between the predictor and dependent variables\n",
    "\n",
    "**USAGE**:\n",
    "[] = IMPORTANCE_SAMPLER(RAW_DATA, ANALYSIS_SETTINGS)\n",
    "\n",
    "**INPUTS**:\n",
    "- raw_data: The data matrix (total number of trials x 6 columns). Refer to RUN_IMPORTANCE_SAMPLER()\n",
    "- analysis_settings: A struct that holds algorithm relevant settings. Refer to RUN_IMPORTANCE_SAMPLER()\n",
    "\n",
    "**OUTPUTS**:\n",
    "- Saves a .mat file in current_path/analysis_id/analysis_id_importance_sampler.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "from scipy import optimize\n",
    "import scipy.io as sio\n",
    "from scipy import special\n",
    "\n",
    "def importance_sampler(raw_data, analysis_settings):\n",
    "    time = datetime.datetime.now()\n",
    "    print('Start time {}/{} {}:{}'.format(time.month, time.day, time.hour, time.minute))\n",
    "    \n",
    "    #R esetting the random number seed based on the clock\n",
    "    random.seed()\n",
    "    \n",
    "    # Preprocessing the data matrix and updating the analysis_settings struct with additional/missing information\n",
    "    preprocessed_data, ana_opt = preprocessing_setup(raw_data, analysis_settings)\n",
    "    \n",
    "    # Housekeeping\n",
    "    importance_sampler = {} # Creating the output struct\n",
    "    hold_betas_per_iter = np.full((ana_opt['em_iterations']+1, 2), np.nan) # Matrix to hold the betas over em iterations\n",
    "    exp_max_fval = np.full((ana_opt['em_iterations'], 1), np.nan) # Matrix to hold the fvals over em iterations\n",
    "    normalized_w = np.full((ana_opt['em_iterations']+1, ana_opt['particles']), np.nan) # Vector to hold the normalized weights\n",
    "    global tau\n",
    "    global bounds\n",
    "    global w\n",
    "    global net_effects\n",
    "    global dependent_var\n",
    "    \n",
    "    # fetch parameters\n",
    "    tau = ana_opt['tau'] # Store the tau for convenience\n",
    "    bounds = family_of_curves(ana_opt['curve_type'], 'get_bounds') # Get the curve parameter absolute bounds\n",
    "    nParam = family_of_curves(ana_opt['curve_type'], 'get_nParams') # Get the number of curve parameters\n",
    "    hold_betas = [ana_opt['beta_0'], ana_opt['beta_1']] # Store the betas into a vector\n",
    "    \n",
    "    for em in range(ana_opt['em_iterations']): # for every em iteration\n",
    "        hold_betas_per_iter[em, :] = hold_betas # Store the logreg betas over em iterations\n",
    "        print('Betas: %0.4f, %0.4f'.format(hold_betas[0], hold_betas[1]))\n",
    "        print('EM Iteration: %d'.format(em))\n",
    "\n",
    "        # Initialize the previous iteration curve parameters, weight vector, net_effects and dependent_var matrices\n",
    "        prev_iter_curve_param = np.full((ana_opt['particles'], family_of_curves(ana_opt.curve_type, 'get_nParams')), np.nan) # Matrix to hold the previous iteration curve parameters\n",
    "        w = np.full((ana_opt['particles']), np.nan) # Vector to hold normalized weights\n",
    "        net_effects = np.full((len(ana_opt['net_effect_clusters']), ana_opt['particles']), np.nan) # Matrix to hold the predictor variables (taking net effects if relevant) over all particles\n",
    "        dependent_var = [] # This vector cannot be initialized in advance since we don't know the length of this vector (dropping outliers)\n",
    "        \n",
    "        # Sampling curve parameters\n",
    "        if em == 0 # only for the first em iteration\n",
    "            param = common_to_all_curves(ana_opt['curve_type'], 'initial_sampling', ana_opt['particles'], ana_opt['resolution']) # Good old uniform sampling\n",
    "        else: # for em iterations 2, 3, etc\n",
    "            # Sample curve parameters from previous iteration's curve parameters based on normalized weights\n",
    "            prev_iter_curve_param = param # storing the previous iteration's curve parameters since we need them to compute likelihood\n",
    "            # Here we sample curves (with repetitions) based on the weights\n",
    "            param = prev_iter_curve_param[random.choices(np.arange(ana_opt['particles']), k=ana_opt['particles'], weights=normalized_w[em-1, :]),:]\n",
    "            # Add Gaussian noise since some curves are going to be identical due to the repetitions\n",
    "            # NOISE: Sample from truncated normal distribution using individual curve parameter bounds, mean = sampled curve parameters and sigma = tau\n",
    "            for npm in range(nParam):\n",
    "                param[:, npm] = truncated_normal(bounds[npm, 1], bounds[npm, 2], param[:, npm], tau, ana_opt['particles'])\n",
    "        param = common_to_all_curves(ana_opt['curve_type'], 'check_if_exceed_bounds', param) # Check whether curve parameters lie within the upper and lower bounds\n",
    "        if ana_opt['curve_type'] is 'horz_indpnt':\n",
    "            param = common_to_all_curves(ana_opt['curve_type'], 'sort_horizontal_params', param) # Check if the horizontal curve parameters are following the right trend i.e. x1 < x2\n",
    "        \n",
    "        # Compute the likelihood over all subjects (i.e. log probability mass function if logistic regression)\n",
    "        #  This is where we use the chunking trick II\n",
    "        for ptl_idx in range(np.shape(ana_opt['ptl_chunk_idx'])[0]):\n",
    "            output_struct = family_of_curves(ana_opt['curve_type'], 'compute_likelihood', ana_opt['net_effect_clusters'], ana_opt['ptl_chunk_idx']['ptl_idx, 3'],\n",
    "                                             param[ana_opt['ptl_chunk_idx'][ptl_idx, 1]:ana_opt['ptl_chunk_idx'][ptl_idx, 2], :], hold_betas, preprocessed_data,\n",
    "                                             ana_opt['distribution'], ana_opt['dist_specific_params'], ana_opt['data_matrix_columns'])\n",
    "            \n",
    "            w[ana_opt['ptl_chunk_idx'][ptl_idx, 1]:ana_opt['ptl_chunk_idx'][ptl_idx, 2]] = output_struct['w'] # Gather weights\n",
    "\n",
    "            net_effects[:, ana_opt['ptl_chunk_idx'][ptl_idx, 1]:ana_opt.ptl_chunk_idx[ptl_idx, 2]] = output_struct['net_effects'] # Gather predictor variable\n",
    "            if ptl_idx == 1:\n",
    "                dependent_var = output_struct['dependent_var'] # Gather dependent variable only once, since it is the same across all ptl_idx\n",
    "\n",
    "        if np.any(np.isnan(w)):\n",
    "            raise ValueError('NaNs in normalized weight vector w!')\n",
    "            \n",
    "        # Compute the p(theta) and q(theta) weights\n",
    "        if em > 0:\n",
    "            p_theta_minus_q_theta = compute_weights(ana_opt['curve_type'], ana_opt['particles'], normalized_w[em-1,:], prev_iter_curve_param, param, ana_opt['wgt_chunks'], ana_opt['resolution'])\n",
    "            w += p_theta_minus_q_theta\n",
    "        \n",
    "        w = np.exp(w - np.logsumexp(w, 2)) # Normalize the weights using logsumexp to avoid numerical underflow\n",
    "        normalized_w[em, :] = w # Store the normalized weights\n",
    " \n",
    "        # Optimize betas using fminunc\n",
    "        optimizing_function = family_of_distributions(ana_opt['distribution'], 'fminunc_both_betas', w, net_effects, dependent_var, ana_opt['dist_specific_params'])\n",
    "\n",
    "        result = optimize.minimize(optimizing_function, hold_betas, options={'disp':True})\n",
    "        hold_betas = result.x\n",
    "        fval = result.fun\n",
    "\n",
    "        exp_max_fval[em, 1] = fval # gather the fval over em iterations\n",
    "\n",
    "    hold_betas_per_iter[em+1, :] = hold_betas # Store away the last em iteration betas\n",
    "    print('>>>>>>>>> Final Betas: {}, {} <<<<<<<<<'.format(hold_betas[0], hold_betas[1]))\n",
    "\n",
    "    # Flipping the vertical curve parameters if beta_1 is negative\n",
    "    importance_sampler['flip'] = False\n",
    "    neg_beta_idx = hold_betas[1] < 0\n",
    "    if neg_beta_idx:\n",
    "        print('!!!!!!!!!!!!!!!!!!!! Beta 1 is flipped !!!!!!!!!!!!!!!!!!!!')\n",
    "        hold_betas[1] = hold_betas[1] * -1\n",
    "        param = common_to_all_curves(ana_opt['curve_type'], 'flip_vertical_params', param)\n",
    "        importance_sampler['flip'] = True\n",
    "    \n",
    "    w = np.full((ana_opt['particles']), np.nan) # Clearing the weight vector\n",
    "    w_null_hypothesis = np.full((ana_opt['particles']), np.nan) # Used for a likelihoods ratio test to see if our beta1 value is degenerate\n",
    "    null_hypothesis_beta = [hold_betas[0], 0] # The null hypothesis for the likelihoods ratio test states that our model y_hat = beta_0 + beta_1 * predictor variable is no different than the simpler model y_hat = beta_0 + beta_1 * predictor variable WHERE BETA_1 = ZERO i.e. our model is y_hat = beta_0\n",
    "    \n",
    "    for ptl_idx  in range(np.shape(ana_opt.ptl_chunk_idx)[0]):\n",
    "        output_struct = family_of_curves(ana_opt['curve_type'], 'compute_likelihood', ana_opt['net_effect_clusters'], ana_opt['ptl_chunk_idx'][ptl_idx, 3],\n",
    "                                         param[ana_opt['ptl_chunk_idx'][ptl_idx, 1]:ana_opt['ptl_chunk_idx'][ptl_idx, 2], :], hold_betas, preprocessed_data,\n",
    "                                         ana_opt['distribution'], ana_opt['dist_specific_params'], ana_opt['data_matrix_columns'])\n",
    "        w[ana_opt['ptl_chunk_idx'][ptl_idx, 1]:ana_opt['ptl_chunk_idx'][ptl_idx, 2]] = output_struct['w']\n",
    "\n",
    "    \n",
    "    # this code computes the log likelihood of the data under the null hypothesis i.e. using null_hypothesis_beta instead of hold_betas -- it's \"lazy\" because, unlike the alternative hypothesis, we don't have to compute the data likelihood for each particle because it's exactly the same for each particle (b/c compute_likelihood uses z = beta_1 * x + beta_0, but (recall that our particles control the value of x in this equation) beta_1 is zero for the null hypothesis) that's why we pass in the zero vector representing a single particle with irrelevant weights so we don't have to do it for each particle unnecessarily\n",
    "    output_struct_null_hypothesis_lazy = family_of_curves(ana_opt['curve_type'], 'compute_likelihood', ana_opt['net_effect_clusters'], 1, [0 0 0 0 0 0], null_hypothesis_beta, preprocessed_data, ana_opt['distribution'], ana_opt['dist_specific_params'], ana_opt['data_matrix_columns'])\n",
    "    data_likelihood_null_hypothesis = output_struct_null_hypothesis_lazy['w']\n",
    "    data_likelihood_alternative_hypothesis = w\n",
    "\n",
    "    w = w + p_theta_minus_q_theta\n",
    "    if np.any(np.isnan(w)):\n",
    "        raise ValueError('NaNs in normalized weight vector w!')\n",
    "\n",
    "    w = np.exp(w - np.logsumexp(w, 2)) # Normalize the weights using logsumexp to avoid numerical underflow\n",
    "    normalized_w[em+1, :] = w # Store the normalized weights\n",
    "    \n",
    "    # Added for debugging chi-sq, might remove eventually\n",
    "    importance_sampler['data_likelihood_alternative_hypothesis'] = data_likelihood_alternative_hypothesis\n",
    "    importance_sampler['data_likelihood_null_hypothesis'] = data_likelihood_null_hypothesis\n",
    "\n",
    "    # we calculate the data_likelihood over ALL particles by multiplying the data_likelihood for each particle by that particle's importance weight\n",
    "    dummy_var, importance_sampler['likratiotest'] = likratiotest(w * np.transpose(data_likelihood_alternative_hypothesis), data_likelihood_null_hypothesis, 2, 1)\n",
    "    \n",
    "    if np.any(np.isnan(normalized_w)):\n",
    "        raise ValueError('NaNs in normalized weights vector!')\n",
    "    if np.any(np.isnan(exp_max_fval)):\n",
    "        raise ValueError('NaNs in Expectation maximilzation fval matrix!')\n",
    "    if np.any(np.isnan(hold_betas_per_iter)):\n",
    "        raise ValueError('NaNs in hold betas matrix!')\n",
    "\n",
    "    importance_sampler['normalized_weights'] = normalized_w\n",
    "    importance_sampler['exp_max_fval'] = exp_max_fval\n",
    "    importance_sampler['hold_betas_per_iter'] = hold_betas_per_iter\n",
    "    importance_sampler['curve_params'] = param\n",
    "    importance_sampler['analysis_settings'] = ana_opt\n",
    "    \n",
    "    if ana_opt['bootstrap']:\n",
    "        sio.savemat('{}/{}_b{}_importance_sampler.mat'.format(ana_opt['target_dir'], ana_opt['analysis_id'], ana_opt['bootstrap_run']), {'importance_sampler':importance_sampler})\n",
    "    elif ana_opt['scramble']:\n",
    "        sio.savemat('{}/{}_s{}_importance_sampler.mat'.format(ana_opt['target_dir'], ana_opt['analysis_id'], ana_opt['scramble_run']), {'importance_sampler':importance_sampler})\n",
    "    else:\n",
    "        sio.savemat('{}/{}_importance_sampler.mat'.format(ana_opt['target_dir'], ana_opt['analysis_id']), {'importance_sampler':importance_sampler})\n",
    "    print('Results are stored in be stored in {}'.format(ana_opt['target_dir']))\n",
    "\n",
    "    time = datetime.datetime.now()\n",
    "    print('Finish time {}/{} {}:{}'.format(time.month, time.day, time.hour, time.minute))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute_weights\n",
    "To compute (P_theta - Q_theta)\n",
    "\n",
    "**USAGE**:\n",
    "[p_theta_minus_q_theta] = compute_weights(curve_name, nParticles, normalized_w, prev_iter_curve_param, param, wgt_chunks, resolution)\n",
    "\n",
    "**INPUTS**:\n",
    "- curve_name: Name of the family of curves (explicitly passed in)\n",
    "- nParticles: Number of particles to be used (explicitly passed in)\n",
    "- normalized_w: Previous iteration's normalized weights\n",
    "- prev_iter_curve_param: Curve parameters held for the previous iteration\n",
    "- param: Curve parameters held for the current iteration\n",
    "- wgt_chunks: Size of chunk. Purely for limited RAM purposes we break up the giant matrix into smaller matrices to compute the weights (explicitly passed in)\n",
    "- resolution: Resolution to which the activations are rounded of\n",
    "\n",
    "**OUTPUTS**:\n",
    "- p_theta_minus_q_theta: Vector of length P (particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(curve_name, nParticles, normalized_w, prev_iter_curve_param, param, wgt_chunks, resolution):\n",
    "    global which_param\n",
    "    total_vol = common_to_all_curves(curve_name, 'curve_volumes', resolution) # Get the curve volumes (Lesbesgue measure)\n",
    "    nParam = family_of_curves(curve_name, 'get_nParams') # Get the number of curve parameters\n",
    "\n",
    "    # Computing q(theta), i.e. what is the probability of a curve given all curves from the previous iteration\n",
    "    # P(theta|old_theta)\n",
    "    q_theta = np.zeros((nParticles, 1))\n",
    "    reduced_nParticles = nParticles / wgt_chunks\n",
    "    reduced_nParticles_idx = np.concatenate(np.arange(0, reduced_nParticles, nParticles), np.arange(0, reduced_nParticles, nParticles)+reduced_nParticles-1)\n",
    "\n",
    "    for idx in range(np.shape(reduced_nParticles_idx)[1]):\n",
    "        prob_grp_lvl_curve = np.zeros((nParticles, reduced_nParticles))\n",
    "        target_indices = np.arange(reduced_nParticles_idx[1, idx], reduced_nParticles_idx[2, idx])\n",
    "        for npm in range(nParam):\n",
    "            which_param = npm\n",
    "            nth_grp_lvl_param = np.repmat(param[:, npm], 1, reduced_nParticles)\n",
    "            nth_prev_iter_curve_param = np.transpose(prev_iter_curve_param[target_indices, npm])\n",
    "            prob_grp_lvl_curve = np.add(prob_grp_lvl_curve, np.vectorize(compute_trunc_likes)(nth_grp_lvl_param, nth_prev_iter_curve_param))\n",
    "\n",
    "        if np.any(np.isnan(prob_grp_lvl_curve)):\n",
    "            raise ValueError('NaNs in probability of group level curves matrix!')\n",
    "            \n",
    "        q_theta = np.add(q_theta, (np.exp(prob_grp_lvl_curve) * np.transpose(normalized_w[target_indices])))\n",
    "\n",
    "    if np.any(np.isnan(q_theta)):\n",
    "        raise ValueError('NaNs in q_theta vector!')\n",
    "    \n",
    "    # Computing p(theta) prior i.e. what is the probability of a curve in the curve space\n",
    "    p_theta = np.ones((nParticles, 1))\n",
    "    p_theta = np.multiply(p_theta, (1 / total_vol))\n",
    "    if len(np.unique(p_theta)) != 1:\n",
    "        raise ValueError('p_theta is NOT unique!')\n",
    "    if np.any(np.isnan(p_theta)):\n",
    "        raise ValueError('NaNs in p_theta vector!')\n",
    "\n",
    "    p_theta_minus_q_theta = np.transpose(np.log(p_theta)) - np.transpose(np.log(q_theta));\n",
    "    return p_theta_minus_q_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute_weights\n",
    "To compute (P_theta - Q_theta)\n",
    "\n",
    "**USAGE**:\n",
    "[log_likelihood] = compute_trunc_likes(x, mu)\n",
    "\n",
    "**INPUTS**:\n",
    "- x: Kth curve parameter in the current iteration\n",
    "- mu: All curve parameters from the previous iteration\n",
    "- tau: Similar to sigma in Gaussian distribution (via global)\n",
    "- bounds and which_param: Are used to fetch the respective curve parameter's absolute abounds (via global). This is required since we are computing likelihoods for truncated normal\n",
    "\n",
    "NOTE: tau and bounds are NOT modified throughout this code once initially set \n",
    "\n",
    "**OUTPUTS**:\n",
    "- log_likelihood of that curve given all the P curves from the previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trunc_likes(x, mu):\n",
    "    global tau, bounds, which_param\n",
    "\n",
    "    if tau <= 0:\n",
    "        raise ValueError('Tau is <= 0!')\n",
    "\n",
    "    # This ugly thing below is a manifestation of log(1 ./ (tau .* (normcdf((bounds(which_param, 2) - mu) ./ tau) - normcdf((bounds(which_param, 1) - mu) ./ tau))) .* normpdf((x - mu) ./ tau))\n",
    "    # Refer to http://en.wikipedia.org/wiki/Truncated_normal_distribution for the truncated normal distribution\n",
    "    log_likelihood =  -(np.log(tau) + np.log(np.multiply(0.5, special.erfc(-(\n",
    "        np.divide(bounds[which_param, 2]-mu,np.multiply(tau, math.sqrt(2)))))))) + (\n",
    "        np.multiply(-0.5, np.log(2)+np.log(np.pi)) - (np.multiply(0.5, np.power(np.divide(x-mu, tau), 2))))\n",
    "    return log_likelihood"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
