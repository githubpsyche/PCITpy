{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing_setup(data, analysis_settings)\n",
    "To peform sanity checks on the input data and the algorithm parameter struct. Massage the data (i.e. drop outliers, zscore data, etc)\n",
    "\n",
    "**USAGE**:\n",
    "[OUTPUT] = FAMILY_OF_CURVES(CURVE_TYPE, GET_INFO, VARARGIN)\n",
    "\n",
    "**INPUTS**:\n",
    "- data: Input data matrix (total number of trials x 6 columns)\n",
    "- analysis_settings: Struct with algorithm parameters\n",
    "\n",
    "**OUTPUTS**:\n",
    "- data: Input data matrix (if applicable, outlier free, zscored, category specific data only, etc)\n",
    "- analysis_settings: Struct with algorithm parameters; some additional parameters are added to this struct as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def preprocessing_setup(data, analysis_settings):\n",
    "    print('********** START OF MESSAGES **********')\n",
    "    \n",
    "    # Checks if the data matrix has 6 columns\n",
    "    nCols = np.shape(data)[1]\n",
    "    if nCols != 6:\n",
    "        raise ValueError('Incorrect number of columns ({}) in the input matrix!'.format(nCols))\n",
    "    \n",
    "    # Registering which column in the data matrix is carrying which piece of information\n",
    "    if 'data_matrix_columns' in analysis_settings:\n",
    "        \n",
    "    if (not ('data_matrix_columns' in analysis_settings)) or (not analysis_settings['data_matrix_columns']):\n",
    "        # Setting it to the default\n",
    "        analysis_settings['data_matrix_columns'] = struct();\n",
    "        analysis_settings['data_matrix_columns']['subject_id'] = 0\n",
    "        analysis_settings['data_matrix_columns']['trials'] = 1\n",
    "        analysis_settings['data_matrix_columns']['category'] = 2\n",
    "        analysis_settings['data_matrix_columns']['predictor_var'] = 3\n",
    "        analysis_settings['data_matrix_columns']['dependent_var'] = 4\n",
    "        analysis_settings['data_matrix_columns']['net_effect_clusters'] = 5\n",
    "    \n",
    "    subject_id_column = analysis_settings['data_matrix_columns']['subject_id']\n",
    "    trials_column = analysis_settings['data_matrix_columns']['trials']\n",
    "    category_column = analysis_settings['data_matrix_columns']['category']\n",
    "    predictor_var_column = analysis_settings['data_matrix_columns']['predictor_var']\n",
    "    dependent_var_column = analysis_settings['data_matrix_columns']['dependent_var']\n",
    "    net_effect_clusters_column = analysis_settings['data_matrix_columns']['net_effect_clusters']\n",
    "    \n",
    "    # Checks if the em iterations is specified; if not specified then it is set to a default of 20\n",
    "    if (not ('em_iterations' in analysis_settings)) or (analysis_settings['em_iterations'] <= 0):\n",
    "        analysis_settings['em_iterations'] = 20\n",
    "        print('Missing number of iterations! It is set to a default of {}'.format(analysis_settings['em_iterations']))\n",
    "    \n",
    "    # Checks if the no. of particles is specified; if not specified then it is set to a default of 1000\n",
    "    if (not ('particles' in analysis_settings)) or (analysis_settings['particles'] <= 0):\n",
    "        analysis_settings['particles'] = 100000\n",
    "        print('Missing number of particles! It is set to a default of {}'.format(analysis_settings['particles']))\n",
    "    \n",
    "    # Checks if the family of curves is specified; if not specified then it is set to a default of 'horz_indpnt' (Refer to family of curves)\n",
    "    if (not ('curve_type' in analysis_settings)) or (not analysis_settings['curve_type']):\n",
    "        analysis_settings['curve_type'] = 'horz_indpnt';\n",
    "        print('Missing family of curves! It is set to a default of {}'.format(analysis_settings['curve_type']))\n",
    " \n",
    "    # Checks if the family of curves exist by fetching the number of curve parameters. This is just a sanity check\n",
    "    if  not isinstance(family_of_curves(analysis_settings['curve_type'], 'get_nParams'), int)\n",
    "        raise ValueError('{} - Does not exist! Check family_of_curves.m script'.format(analysis_settings['curve_type']))\n",
    "    \n",
    "    # Checks if the distribution is specified;\n",
    "    # If not specified and if the dependent variable is binary then it is set to 'bernoulli'; otherwise it is set to to 'normal'\n",
    "    if (not ('distribution' in analysis_settings)) or (not analysis_settings['distribution']):\n",
    "        if len(np.unique(data[:, dependent_var_column])) == 2:\n",
    "            analysis_settings['distribution'] = 'bernoulli'\n",
    "        else\n",
    "            analysis_settings['distribution'] = 'normal'\n",
    "        print('Missing distribution! based on the dependent variable it is set to {}'.format(analysis_settings['distribution']))\n",
    "        \n",
    "    # Checks if the distribution specific parameters exist\n",
    "    if (not ('dist_specific_params' in analysis_settings)) or (not analysis_settings['dist_specific_params']):\n",
    "        if analysis_settings['distribution'] is 'bernoulli':\n",
    "            # For a Bernoulli dist there are no parameters so it is empty. We still need the struct to exist\n",
    "            analysis_settings['dist_specific_params'] = {}\n",
    "        elif analysis_settings['distribution'] is 'normal':\n",
    "            # For normal distribution the additional parameter is sigma. We pass in sigma here.\n",
    "            analysis_settings['dist_specific_params'] = {}\n",
    "            analysis_settings['dist_specific_params']['sigma'] = 1 # Default is 1\n",
    "            print('Missing sigma for normal distribution! It is set to {}'.format(analysis_settings['dist_specific_params']['sigma']))\n",
    "        else:\n",
    "            analysis_settings['dist_specific_params'] = {}\n",
    "    \n",
    "    # Checks if normal distribution specific parameter is valid i.e. sigma > 0\n",
    "    if (analysis_settings['distribution'] is 'normal') & (analysis_settings['dist_specific_params']['sigma'] <= 0):\n",
    "        raise ValueError('Normal distribution sigma will need to > 0! sigma = {}'.format(analysis_settings['dist_specific_params']['sigma']))\n",
    "    \n",
    "    # Checks if beta_0 is specified; if not specified then it is set to a default of 0\n",
    "    if not('beta_0' in analysis_settings):\n",
    "        analysis_settings['beta_0'] = 0\n",
    "        print('Missing initial setting for beta_0! It is set to a default of {}'.format(analysis_settings['beta_0']))\n",
    "        \n",
    "    # Checks if beta_1 is specified; if not specified then it is set to a default of 1\n",
    "    if not('beta_1' in analysis_settings):\n",
    "        analysis_settings['beta_1'] = 1\n",
    "        print('Missing initial setting for beta_1! It is set to a default of {}'.format(analysis_settings['beta_1']))\n",
    "\n",
    "    # Checks if tau is specified; if not specified then it is set to a default of 0.05\n",
    "    if not('tau' in analysis_settings):\n",
    "        analysis_settings['tau'] = 0.05\n",
    "        print('Missing initial setting for tau! It is set to a default of {}'.format(analysis_settings['tau']))\n",
    "\n",
    "    # Checks if this is a bootstrap run; if not specified then it is set to a default of false\n",
    "    if not('bootstrap' in analysis_settings):\n",
    "        analysis_settings['bootstrap'] = False\n",
    "        print('Missing initial setting for beta_1! It is set to a default of {}'.format(analysis_settings['bootstrap']))\n",
    "\n",
    "    # Checks if bootstrap flag is boolean\n",
    "    if not (type(analysis_settings['bootstrap']) is bool):\n",
    "        raise ValueError('analysis_settings.bootstrap field will need to be boolean!')\n",
    "    \n",
    "    # Checks if this is a scramble run; if not specified then it is set to a default of false\n",
    "    if not('scramble' in analysis_settings):\n",
    "        analysis_settings['scramble'] = False\n",
    "        \n",
    "    # Checks if scramble flag is boolean\n",
    "    if not (type(analysis_settings['scramble']) is bool):\n",
    "        raise ValueError('analysis_settings.scramble field will need to be boolean!')\n",
    "\n",
    "    # Errors if both bootstrap and scramble flags exist\n",
    "    if analysis_settings['scramble'] & analysis_settings['bootstrap']:\n",
    "        raise ValueError('Cannot run both scramble AND bootstrap analyses at the same time! Set any one flag to be false')\n",
    "\n",
    "    # Builds a bootstrap data matrix from the original data matrix\n",
    "    if analysis_settings['bootstrap']:\n",
    "        # We need a bootstrap sample number\n",
    "        if (not ('bootstrap_run' in analysis_settings)) or (not analysis_settings['bootstrap_run']):\n",
    "            raise ValueError('Missing bootstrap sample number! set analysis_settings.bootstrap_run to a valid sample number')\n",
    "        \n",
    "        bootstrap_data = []\n",
    "        new_cluster_count = 1\n",
    "        new_subject_count = 1\n",
    "        \n",
    "        # Get the number of subjects from the data matrix\n",
    "        nSubjects = len(np.unique(data[:, subject_id_column]))\n",
    "        # Randomly sample with replacement the number of subjects thus generating our bootstrap sample\n",
    "        subj_num_with_replacement = random.choices(np.arange(nSubjects), k=nSubjects)\n",
    "        # For each subject in our bootstrap sample gather all relevant information\n",
    "        for i in range(len(subj_num_with_replacement)):\n",
    "            subj_idx = np.where(data[:, subject_id_column] == subj_num_with_replacement[i])\n",
    "\n",
    "            # Recreate a new net effect cluster since this will need to be unique in the data matrix\n",
    "            # (by repeatedly sampling subjects we could be repeating the net effect clusters)\n",
    "            cluster_vector = data[subj_idx, net_effect_clusters_column]\n",
    "            cluster_numbers = np.unique[cluster_vector]\n",
    "            for j in range(len(cluster_numbers)):\n",
    "                target_idx = np.where(data[subj_idx, net_effect_clusters_column] == cluster_numbers[j])\n",
    "                cluster_vector[target_idx] = new_cluster_count\n",
    "                new_cluster_count += 1\n",
    "\n",
    "            # Recreate a new subject id\n",
    "            # (by repeatedly sampling subjects we could be repeating the subject id's)\n",
    "            # Gather all information into a bootstrap_data matrix\n",
    "            bootstrap_data.append(np.concatenate(np.repmat(new_subject_count, len(subj_idx), 1), data[subj_idx,trials_column:dependent_var_column], cluster_vector))\n",
    "            new_subject_count += 1\n",
    "\n",
    "        # Perform some sanity checks to ensure that the bootstrap_data matrix is similar to the actual data matrix\n",
    "        if not (np.shape(bootstrap_data) == np.shape(data)):\n",
    "            raise ValueError('Size of bootstrap dataset NOT the same as original data!')\n",
    "        if not (len(np.unique(data[:, net_effect_clusters_column])) == len(np.unique(bootstrap_data[:, net_effect_clusters_column]))):\n",
    "            raise ValueError('The number of clusters are not the same in the original and bootstrap sample!')\n",
    "        if not np.array_equal(data[:, subject_id_column], bootstrap_data[:, subject_id_column]):\n",
    "            raise ValueError('The ordering of subjects are not the same in the original and bootstrap sample!')\n",
    "\n",
    "        # Store away the bootstrap sample subject information for future reference\n",
    "        analysis_settings['bootstrap_run_subj_id'] = subj_num_with_replacement\n",
    "        data = bootstrap_data\n",
    "\n",
    "    # Checks if this analysis will be need to performed for a specific category; if not specified then it is set to a default of [] i.e. NOT category specific\n",
    "    if not ('category' in analysis_settings):\n",
    "        analysis_settings.category = []\n",
    "        print('Missing category specific analyses information! We are going to ignore the category dimension i.e. all trials from all categories will be analysed')\n",
    "\n",
    "    # If this analysis is to be performed for a specific category then it filters out data from other irrelavant categories\n",
    "    if len(analysis_settings['category']) > 0:\n",
    "        target_cat_idx = []\n",
    "        data_cat = np.unique(data[:, category_column])\n",
    "        for c  in range(len(analysis_settings['category'])):\n",
    "            cat_exist = np.where(data_cat == analysis_settings['category'][c])[0]\n",
    "            if cat_exist.size == 0:\n",
    "                raise ValueError('Category does not exist! You have set analysis_settings.category[{}]={}'.format(c, analysis_settings['category'][c]))\n",
    "            target_cat_idx = np.concatenate(target_cat_idx, np.where(data[:, category_column] == analysis_settings['category'][c])[0])\n",
    "        data = data[target_cat_idx, :]\n",
    "    \n",
    "    # Checks if outliers (i.e. data trials) will need to dropped; if not specified then it is set to a default of 'DO NOT DROP OUTLIERS'\n",
    "    if not ('drop_outliers' in analysis_settings):\n",
    "        analysis_settings['drop_outliers'] = 3\n",
    "        print('Missing drop_outliers specific information! We are dropping outliers that are {} standard deviations away from the group mean'.format(analysis_settings['drop_outliers']))\n",
    "    \n",
    "    # If this analysis requires the outliers to be dropped, then the code below drops the data trials within XXXX standard deviations from the GROUP MEAN\n",
    "    if analysis_settings['drop_outliers'] > 0\n",
    "        # NaN's do not qualify as outliers so we filter them out and add them at the end of this step\n",
    "        nan_free_idx = np.logical_not(np.isnan(data[:, predictor_var_column]))\n",
    "        # NaN free data\n",
    "        nan_free_data = data[nan_free_idx, :]\n",
    "        std_dev_predictor_var = np.std(nan_free_data[:, predictor_var_column], ddof=1) * analysis_settings['drop_outliers']\n",
    "        mean_predictor_var = np.mean(nan_free_data[:, predictor_var_column])\n",
    "        predictor_var_idx = (nan_free_data[:, predictor_var_column] > (mean_predictor_var - std_dev_predictor_var)) & (nan_free_data[:, predictor_var_column] < (mean_predictor_var + std_dev_predictor_var))\n",
    "        print('{} trials are dropped since they are regarded as outliers'.format(np.shape(nan_free_data)[subject_id_column] - np.sum(predictor_var_idx)))\n",
    "        nan_free_data_outlier_dropped = nan_free_data[predictor_var_idx, :]\n",
    "        # NaN's trials\n",
    "        nan_data = data[np.logical_not(nan_free_idx), :]\n",
    "        # Combine the NaN data with the outlier free data\n",
    "        data = np.concatenate(nan_free_data_outlier_dropped, nan_data)\n",
    "    \n",
    "    # Following the 'filter by category' and 'drop outliers', if applicable, we check if the data matrix is empty\n",
    "    nTrials = np.shape(data)[subject_id_column]\n",
    "    if nTrials <= 0:\n",
    "        raise ValueError('No input data!')\n",
    "\n",
    "    # Checks if we need to zscore predictor var within subjects, if not specified then it is set to default of FALSE\n",
    "    if not ('zscore_within_subjects' in analysis_settings):\n",
    "        analysis_settings['zscore_within_subjects'] = 0\n",
    "        print('Missing zscore_within_subjects information! We are NOT zscoring within subjects')\n",
    "\n",
    "    # Verifies if zscore within subjects is boolean\n",
    "    if not (type(analysis_settings['zscore_within_subjects']) is bool):\n",
    "        raise ValueError('zscore_within_subjects field will need to be boolean!')\n",
    "    \n",
    "    # Zscore the predictor variable within each subject\n",
    "    if analysis_settings['zscore_within_subjects']:\n",
    "        # NaN's do not qualify to be zscored\n",
    "        nan_free_idx = np.logical_not(np.isnan(data[:, predictor_var_column]))\n",
    "        # NaN free data\n",
    "        nan_free_data = data[nan_free_idx, :]\n",
    "        # We get the list of subject id's (we use this cell array in zscoring the data within each subject, if applicable)\n",
    "        subject_id_list = np.unique(nan_free_data[:, subject_id_column])\n",
    "        # We get the number of subjects\n",
    "        nSubjects = len(subject_id_list)\n",
    "        if nSubjects <= 0:\n",
    "            raise ValueError('Not valid number of subjects!')\n",
    "        for s in range(nSubjects):\n",
    "            subject_idx = np.where(nan_free_data[:, subject_id_column] == subject_id_list[s])[0]\n",
    "            nan_free_data[subject_idx, predictor_var_column] = stats.zscore(nan_free_data[subject_idx, predictor_var_column], ddof=1)\n",
    "        print('Predictor variables within each subject are zscored!')\n",
    "        # NaN's trials\n",
    "        nan_data = data[np.logical_not(nan_free_idx), :]\n",
    "        # Combine the NaN data with the outlier free data\n",
    "        data = np.concatenate(nan_free_data, nan_data)\n",
    "    \n",
    "    # Checks if the resolution is specified, if not specified then it is set to default of 4. This translates to 1e-4 = 0.0001\n",
    "    if (not ('resolution' in analysis_settings)) or (analysis_settings['resolution'] <= 0):\n",
    "        analysis_settings['resolution'] = 4\n",
    "        print('Missing resolution! It is set to a default of %d'.format(analysis_settings['resolution']))\n",
    "\n",
    "    # if we have normally distributed data, we want to z-score the dependent variable\n",
    "    if analysis_settings['distribution'] is 'normal':\n",
    "        data[:,dependent_var_column] = stats.zscore(data[:, dependent_var_column], ddof=1)\n",
    "\n",
    "    # We scale the predictor var to be between 0 and 1 and round it to 4 digits\n",
    "    nan_free_idx = np.logical_not(np.isnan(data[:, predictor_var_column]))\n",
    "    nan_free_data = data[nan_free_idx, :]\n",
    "    nan_free_data[:, predictor_var_column] = round_to(scale_data(nan_free_data[:, predictor_var_column], 0, 1), analysis_settings['resolution'])\n",
    "    nan_data = data[np.logical_not(nan_free_idx), :]\n",
    "    data = np.concatenate(nan_free_data, nan_data)\n",
    "    \n",
    "    # Scrambling the data matrix\n",
    "    if analysis_settings['scramble']:\n",
    "        if (not ('scramble_run' in analysis_settings)) or (not analysis_settings['scramble_run']):\n",
    "            raise ValueError('Missing scramble sample number! set analysis_settings.scramble_run to a valid sample number')\n",
    "        if (not ('scramble_style' in analysis_settings)) or (not analysis_settings['scramble_style']):\n",
    "            analysis_settings['scramble_style'] = 'within_subjects_within_categories' # The most conservative among all scramble techniques\n",
    "            print('Missing scramble style! It is set a default of {}'.format(analysis_settings['scramble_style']))\n",
    "\n",
    "        # We get the list of subject id's\n",
    "        subject_id_list = np.unique(data[:, subject_id_column])\n",
    "        # We get the number of subjects in this analysis\n",
    "        nSubjects = len(subject_id_list)\n",
    "        if nSubjects <= 0:\n",
    "            raise ValueError('Not valid number of subjects!')\n",
    "\n",
    "        if analysis_settings['scramble_style'] is 'within_subjects_within_categories':\n",
    "            # Here we scramble all dependent variables WHILE respecting the net effect boundaries, subject groupings and category groupings\n",
    "            categories = np.unique(data[:, category_column])\n",
    "            for s in range(nSubjects):\n",
    "                for c in range(len(categories)):\n",
    "                    subject_category_idx = np.where((data[:, subject_id_column] == subject_id_list[s]) & (data[:, category_column] == categories[c]))[0]\n",
    "                    if len(subject_category_idx) > 1:\n",
    "                        data[subject_category_idx, dependent_var_column] = scramble_dependent_variable(\n",
    "                            data[subject_category_idx, dependent_var_column], data[subject_category_idx, net_effect_clusters_column])\n",
    "\n",
    "        elif analysis_settings['scramble_style'] is 'within_subjects_across_categories':\n",
    "            # Here we scramble all dependent variables WHILE respecting the net effect boundaries and subject groupings\n",
    "            for s in range(nSubjects):\n",
    "                subject_idx = np.where(data[:, subject_id_column] == subject_id_list[s])[0]\n",
    "                if len(subject_idx) > 1:\n",
    "                    data[subject_idx, dependent_var_column] = scramble_dependent_variable(\n",
    "                        data[subject_idx, dependent_var_column], data[subject_idx, net_effect_clusters_column])\n",
    "\n",
    "        elif analysis_settings['scramble_style'] is 'across_subjects_across_categories':\n",
    "            # Here we scramble all dependent variables WHILE respecting the net effect boundaries\n",
    "            all_idx = np.arange(np.shape(data)[0])\n",
    "            if len(all_idx) > 1:\n",
    "                data[all_idx, dependent_var_column] = scramble_dependent_variable(\n",
    "                    data[all_idx, dependent_var_column], data[all_idx, net_effect_clusters_column])\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Invalid analysis_settings.scramble_style={}'.format(analysis_settings['scramble_style']))\n",
    "    \n",
    "    # Our data matrix looks like data = [subject id, item, category, predictor var, dependent var, net effect cluster]\n",
    "    # We verify if the subject id and dependent var columns are unique for the net effect clusters\n",
    "    # Below is a example of a valid data matrix (note dependent variable is unique within net effect cluster 111)\n",
    "    # data(1, :) = [24, 1, 1, 0.3333, 0, 111]\n",
    "    # data(2, :) = [24, 2, 2, 0.2222, 0, 111]\n",
    "    # data(3, :) = [24, 3, 1, 0.4444, 0, 111]\n",
    "    # Below is a example of an invalid data matrix (note dependent variable is not unique within net effect cluster 111)\n",
    "    # data(1, :) = [24, 1, 1, 0.3333, 0, 111]\n",
    "    # data(2, :) = [24, 2, 2, 0.2222, 1, 111]\n",
    "    # data(3, :) = [24, 3, 1, 0.4444, 0, 111]\n",
    "\n",
    "    # Fetching the net effect clusters\n",
    "    net_effect_clusters = np/unique(data[:, net_effect_clusters_column])\n",
    "    analysis_settings['net_effect_clusters'] = net_effect_clusters\n",
    "\n",
    "    # If net effect clusters exist verify if the Subject Id and dependent variable are unique for those clusters\n",
    "    if len(net_effect_clusters) != np.shape(data)[0]:\n",
    "        for i in range(len(net_effect_clusters)):\n",
    "            cluster_idx = np.where(data[:, net_effect_clusters_column] == net_effect_clusters[i])[0]\n",
    "            if np.shape(np.unique(data[cluster_idx, [subject_id_column, dependent_var_column]], axis=0))[0] != 1:\n",
    "                raise ValueError('Subject Id and/or dependent variable not unique for net effect cluster {}! Check the data matrix'.format(net_effect_clusters[i]))\n",
    "    else:\n",
    "        # If net effect clusters DO NOT exist then we treat each row as a net effect cluster by itself\n",
    "        print('Each row will be treated separately. We will NOT be computing the net effect of any rows')\n",
    "\n",
    "    # We create an analysis id unique to this analysis\n",
    "    if (not ('analysis_id' in analysis_settings)) or (not analysis_settings['analysis_id']):\n",
    "        time = datetime.datetime.now()\n",
    "        analysis_settings['analysis_id'] = '{}-{}-{}-{}-{}'.format(time.month, time.day, time.hour, time.minute, time.second)\n",
    "    \n",
    "    # We create a results directory if no specific target directory is mentioned\n",
    "    if (not ('target_dir' in analysis_settings)) or (not analysis_settings['target_dir']):\n",
    "        results_dir = os.path.join(os.getcwd(), 'results')\n",
    "        if not os.path.isdir(results_dir):\n",
    "            os.mkdir(results_dir)\n",
    "        analysis_settings['target_dir'] = results_dir\n",
    "        \n",
    "    # target_directory = 'results/analysis_id'\n",
    "    analysis_settings['target_dir'] = os.path.join(analysis_settings['target_dir'], analysis_settings['analysis_id'])\n",
    "    if not os.path.isdir(analysis_settings['target_dir']):\n",
    "        os.mkdir(analysis_settings['target_dir'])\n",
    "        \n",
    "    # Due to memory constraints we perform two chunking tricks\n",
    "\n",
    "    # Chunking trick I\n",
    "    # In the curve fitting algorithm we need to compute the p(current iteration curves | previous iteration curves). This matrix is huge when the number of particles (curves) is \n",
    "    # large, say 100,000. Even with a 8 Gb RAM, dedicated to Matlab, we still get a out of memory errors. To avoid this problem we chunk the matrix into smaller, more manageable matrices.\n",
    "    # Setting the chunk size to be particles x 0.05 -> 100,000 x 0.05 = 5000, translstes to p(current iteration curves(5000 curves at a time) | previous iteration curves).\n",
    "    analysis_settings['wgt_chunks'] = analysis_settings['particles'] * 0.05\n",
    "    # If the chunk size is less then 5000 we set it be the number of particles itself\n",
    "    if analysis_settings['wgt_chunks'] < 5000:\n",
    "        analysis_settings['wgt_chunks'] = analysis_settings['particles']\n",
    "\n",
    "    # Chunking trick II\n",
    "    if (not ('particle_chunks' in analysis_settings)):\n",
    "        analysis_settings['particle_chunks'] = 2\n",
    "        print('Missing particle chunks! It is set to a default of {}'.format(analysis_settings['particle_chunks']))\n",
    "    # Depending on the number of particle chunks we get start, end points and the number of particles within each chunk. For instance 1000 particles divided into 4 chunks will look like,\n",
    "    # 1\t250\t250\n",
    "    # 251\t500\t250\n",
    "    # 501\t750\t250\n",
    "    # 751\t1000\t250\n",
    "    analysis_settings['ptl_chunk_idx'][:, 1] = np.arange(0, analysis_settings['particles']/analysis_settings['particle_chunks'], analysis_settings['particles'])\n",
    "    analysis_settings['ptl_chunk_idx'][:, 2] = np.concatenate(analysis_settings['ptl_chunk_idx'][2:, 1] - 1, analysis_settings['particles'])\n",
    "    analysis_settings['ptl_chunk_idx'][:, 3] = analysis_settings['ptl_chunk_idx'][:, 2] - analysis_settings.ptl_chunk_idx[:, 1] + 1\n",
    "\n",
    "    # Storing analysis relevant information into the analysis_settings struct\n",
    "    # We get the list of subject id's\n",
    "    subject_id_list = np.unique(data[:, subject_id_column])\n",
    "    # We get the number of subjects in this analysis\n",
    "    analysis_settings['nSubjs'] = len(subject_id_list)\n",
    "    if analysis_settings['nSubjs'] <= 0:\n",
    "        raise ValueError('Not valid number of subjects!')\n",
    "\n",
    "    print('********** END OF MESSAGES **********')\n",
    "    return data, analysis_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scramble_dependent_variable(target_dependent_variables, target_net_effect_clusters)\n",
    "To take a dependent variable vector and scramble it such that the net effect cluster groupings are NOT violated\n",
    "\n",
    "**USAGE**:\n",
    "[OUTPUT] = FAMILY_OF_CURVES(CURVE_TYPE, GET_INFO, VARARGIN)\n",
    "\n",
    "**INPUTS**:\n",
    "- target_dependent_variables: The vector you would like scrambled\n",
    "- target_net_effect_clusters: The groupings that you would like to NOT violate. Follow the example below\n",
    "\n",
    "**OUTPUTS**:\n",
    "- scrambled_vector: A scrambled vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scramble_dependent_variable(target_dependent_variables, target_net_effect_clusters):\n",
    "    if np.logical_not(np.shape(target_dependent_variables) == np.shape(target_net_effect_clusters)):\n",
    "        raise ValueError('Size of input vectors must be the same!')\n",
    "    \n",
    "    # Detailed example\n",
    "    # example data matrix: target_dependent_variables = [1, 0, 1, 0, 0, 0, 1] and target_net_effect_clusters = [3, 5, 3, 7, 7, 5, 8]\n",
    "\n",
    "    # Fetch the sorted list of net effect clusters and their respective locations\n",
    "    # e.g. for [3, 5, 3, 7, 7, 5, 8] will return [3, 3, 5, 5, 7, 7, 8] and [1, 3, 2, 6, 4, 5, 7]\n",
    "    sorted_neteff_clusters = np.sort(target_net_effect_clusters)\n",
    "    sorted_neteff_clusters_indices = np.argsort(target_net_effect_clusters)\n",
    "    just_ones = np.ones(np.shape(sorted_neteff_clusters)) # Populate a vector full of ones\n",
    "    # compute the length of each net effect cluster\n",
    "    # e.g. for [3, 5, 3, 7, 7, 5, 8] will return [2, 2, 2, 1] i.e. 3 is repeated twice and so on\n",
    "    length_each_neteff_cluster = np.transpose(np.bincount(sorted_neteff_clusters))\n",
    "    length_each_neteff_cluster = length_each_neteff_cluster[length_each_neteff_cluster.astype(bool)]\n",
    "    \n",
    "    # Get the unique list of clusters (i.e. excluding repetitions if any) e.g. [3, 5, 7, 8]\n",
    "    unique_neteff_clusters, unique_indices = np.unique(target_net_effect_clusters, return_index=True)\n",
    "    # Get the associated dependent variables (one per cluster; recall it is unique within a cluster) e.g. [1, 0, 0, 1]\n",
    "    associated_dependent_variables = target_dependent_variables[unique_indices]\n",
    "    # scramble the dependent variables e.g. [0, 0, 1, 1]\n",
    "    scrambled_indices = np.random.permutation(len(associated_dependent_variables))\n",
    "    scrambled_dependent_variables = associated_dependent_variables[scrambled_indices]\n",
    "\n",
    "    # Now we will need to repeat each scrambled dependent variable for the length of that net effect cluster. The next three lines will result in\n",
    "    # [0, 0, 0, 0, 1, 1, 1] corresponding to [3, 3, 5, 5, 7, 7, 8] since the scrambled dependent variable looks like [0, 0, 1, 1] for [3, 5, 7, 8]\n",
    "    cumsum_clutsers = np.cumsum(length_each_neteff_cluster)\n",
    "    indicator_vector = np.zeros((1, cumsum_clutsers[-1]))\n",
    "    indicator_vector[[0] + [cumsum_clutsers[:-1]]] = 1\n",
    "\n",
    "    # Store the scrambled dependent variable in the respective cluster locations\n",
    "    # The original vector looked like [3, 5, 3, 7, 7, 5, 8] so the scrambled vector will look like [0, 0, 0, 1, 1, 0, 1]\n",
    "    scrambled_vector = np.full(np.shape(sorted_neteff_clusters_indices), np.nan)\n",
    "    scrambled_vector[sorted_neteff_clusters_indices] = scrambled_dependent_variables[np.cumsum(indicator_vector)]\n",
    "\n",
    "    if np.any(np.isnan(scrambled_vector)):\n",
    "        raise ValueError('Nan''s in scrambled dependent variable vector!')\n",
    "        \n",
    "    return scrambled_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
